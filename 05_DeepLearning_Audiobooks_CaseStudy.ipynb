{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Case Study: Audiobooks \n",
    "#### by Sooyeon Won \n",
    "\n",
    "### Keywords \n",
    "- Deep Learning \n",
    "- TensorFlow2 - Keras\n",
    "- Unbalanced Data\n",
    "- Classification Problem\n",
    "\n",
    "\n",
    "### Contents \n",
    "\n",
    "<ul>    \n",
    "<li><a href=\"#Introduction\">1.  Introduction</a></li>\n",
    "<li><a href=\"#Preprocessing\">2.  Data Preprocessing</a></li>\n",
    "<li><a href=\"#Analysis\">3.  Data Analysis</a></li>\n",
    "<li><a href=\"#Test\">4.  Test the Model</a></li>\n",
    "</ul>\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Introduction'></a>\n",
    "### 1. Introduction: Business Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " For this analysis, data from an Audiobook App are provided. Note that it relates to the audio versions of books ONLY. Each customer in the database has made a purchase at least once. Based on the available data, I created a machine learning (ML) algorithm that can predict whether customers will re-purchase products from the Audiobook company. <br><br>\n",
    "From the company's perspective, if a customer has a low probability of re-purchasing, the company can reduce costs on advertising to the customer, so that the company can focus its efforts only on the customers who are likely to convert again. In addition, the ML model can identify the most important metrics for a customer who come back again. Identifying new customers creates value and growth opportunities. <br><br>\n",
    "The given dataset contains following information. \n",
    ">- **Customer ID**\n",
    ">- **Book length overall**: Sum of the minute length of all purchases\n",
    ">- **Book length avg**: Average length in minutes of all purchases\n",
    ">- **Price paid_overall**: Total price of all purchases  \n",
    ">- **Price Paid avg**: Average paid price of all purchases\n",
    ">- **Review**: A binary variable whether the customer left a review\n",
    ">- **Review out of 10**: A review score that the customer left  \n",
    ">- **Total minutes listened**: Total minutes of time is a measure of engagement.\n",
    ">- **Completion**: The percentage of completion of the audiobook. \"Total minutes listened\" is divided by \"Book length overall\". It is ranged from 0 to 1.\n",
    ">- **Support requests**: Total number of support requests such as forgotten password, assistance for using the App, and so on.\n",
    ">- **Last visited - purchase date**: It is also a measure of engagement. The bigger the difference, the gibber the engagement (in days).\n",
    "\n",
    "In this analysis, I used all the features as the inputs, except for **Customer ID**, since it is completely arbitrary. \n",
    "The targets are a binary variable, indicating 0 or 1. I took a period of 2 years in the inputs, and the next 6 months as targets. \n",
    "\n",
    "So, basically, I predict whether a customer will convert in the next 6 months. based on the last 2 years of purchasing patterns and individual engagement. 6 months could be debatable, but it is also reasonable. If customers do not convert after 6 months, chances are they've gone to a competitor or didn't like the Audiobooks way of digesting information.  \n",
    "\n",
    "My solution approach is to create a a machine learning algorithm, which is able to predict whether a customer will buy again. This is a classic classification problem with two classes: will not purchase and will purchase, represented by 0s and 1s, respectively. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Preprocessing'></a>\n",
    "## 2. Data Preprocessing\n",
    "&nbsp; 2. 1. Extract the data from the csv <br>\n",
    "&nbsp; 2. 2. Balance the dataset <br>\n",
    "&nbsp; 2. 3. Standardize the inputs <br>\n",
    "&nbsp; 2. 4. Shuffle the data <br>\n",
    "&nbsp; 2. 5. Split the dataset into train, validation, and test <br>\n",
    "&nbsp; 2. 6. Save the three datasets in a tensor friendly format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 1. Extract the data from the csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the relevant libraries \n",
    "import numpy as np\n",
    "from sklearn import preprocessing # for easier data standardization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "raw_csv_data = np.loadtxt('Audiobooks_data.csv',delimiter=',')\n",
    "\n",
    "# Specfiy inputs and target\n",
    "# Inputs - expect for the the arbitrary customer IDs in the first column, and the target\n",
    "unscaled_inputs_all = raw_csv_data[:,1:-1]\n",
    "# Target - the last column\n",
    "targets_all = raw_csv_data[:,-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 2. Balance the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of target 1s:  2237\n",
      "The number of target 0s:  11847\n",
      "Total targets:  14084\n"
     ]
    }
   ],
   "source": [
    "# Target distribution \n",
    "num_one_targets = int(np.sum(targets_all))\n",
    "print('The number of target 1s: ', num_one_targets)\n",
    "print('The number of target 0s: ', len(targets_all) - num_one_targets)\n",
    "print('Total targets: ', len(targets_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_targets_counter = 0\n",
    "indices_to_remove = []\n",
    "\n",
    "for i in range(targets_all.shape[0]):\n",
    "    if targets_all[i] == 0:\n",
    "        zero_targets_counter += 1\n",
    "        if zero_targets_counter > num_one_targets:\n",
    "            indices_to_remove.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the marked indices marked from the for loop\n",
    "unscaled_inputs_equal_priors = np.delete(unscaled_inputs_all, indices_to_remove, axis=0)\n",
    "targets_equal_priors = np.delete(targets_all, indices_to_remove, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Comment**: First, I count how many targets are 1s, indicating that the number of customers who converted. I set a counter for targets that are 0, meaning that the customer did not convert. To create a \"balanced\" dataset, I simply remove some datapoints from the major class, as the lecturer recommended. Note that there are various ways to deal with imbalanced data. For example I took a SMOTE technique in my previous [Starbucks Capstone Project](https://github.com/SooyeonWon/ML_starbucks_capstone_projects). \n",
    "<br><br>\n",
    "To balance the data, I declare a variable that will count the number of target: 0s. Once the 2 classes are balanced, mark entries where the target is 0. Then I created two variables, containing the inputs, and the targets.I delete all indices marked as \"to remove\" in the loop above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 3. Standardize the inputs\n",
    "> To standarize the inputs increases the accuracy of algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the sklearn library, the inputs are standardized \n",
    "scaled_inputs = preprocessing.scale(unscaled_inputs_equal_priors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 4. Shuffle the data\n",
    "> The collected data is arranged by date. Since I will batch later, I must shuffle the data, so that I can keep the same information but in a random order. Otherwise the data is homogenous inside a batch. On the other hand, between the batches, the batches are heterogenous. So, I used the shuffled indices to shuffle the inputs and targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the indices \n",
    "shuffled_indices = np.arange(scaled_inputs.shape[0])\n",
    "np.random.shuffle(shuffled_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the inputs and target\n",
    "shuffled_inputs = scaled_inputs[shuffled_indices]\n",
    "shuffled_targets = targets_equal_priors[shuffled_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 5. Split the dataset into train, validation, and test\n",
    "> In this analysis, I split the data into 80-10-10 distribution of training, validation, and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the total number of samples\n",
    "samples_count = shuffled_inputs.shape[0]\n",
    "\n",
    "# 80% of the whole data are assigned to training dataset \n",
    "train_samples_count = int(0.8 * samples_count)\n",
    "\n",
    "# 10% of the whole data are assigned to validation dataset \n",
    "validation_samples_count = int(0.1 * samples_count)\n",
    "\n",
    "# Finally, test' dataset contains all remaining data.\n",
    "test_samples_count = samples_count - train_samples_count - validation_samples_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create variables that record the inputs and targets for training. \n",
    "train_inputs = shuffled_inputs[:train_samples_count]\n",
    "train_targets = shuffled_targets[:train_samples_count]\n",
    "\n",
    "# Create variables that record the inputs and targets for validation.\n",
    "validation_inputs = shuffled_inputs[train_samples_count:train_samples_count+validation_samples_count]\n",
    "validation_targets = shuffled_targets[train_samples_count:train_samples_count+validation_samples_count]\n",
    "\n",
    "# Create variables that record the inputs and targets for test. \n",
    "test_inputs = shuffled_inputs[train_samples_count+validation_samples_count:]\n",
    "test_targets = shuffled_targets[train_samples_count+validation_samples_count:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> So far I have balanced the dataset to be 50-50 (for targets 0 and 1), but the training, validation, and test were taken from a shuffled dataset. So it should be also checed if the splited sets are also balanced. Note that whenever I rerun the code, I get unequal values, since they are randomly shuffled. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: Target=1: 1796  / Total # targets: 3579  / Percentage: 0.502\n",
      "Validation set: Target=1: 216  / Total # targets: 447  / Percentage: 0.483\n",
      "Test set: Target=1: 225  / Total # targets: 448  / Percentage: 0.502\n"
     ]
    }
   ],
   "source": [
    "# Print the number of targets that are 1s, the total number of samples, and the proportion for each splited dataset.\n",
    "print('Training set: Target=1:', int(np.sum(train_targets)), ' / Total # targets:', int(train_samples_count), ' / Percentage:', np.round(np.sum(train_targets) / train_samples_count,3))\n",
    "print('Validation set: Target=1:', int(np.sum(validation_targets)), ' / Total # targets:', int(validation_samples_count),' / Percentage:',  np.round(np.sum(validation_targets) / validation_samples_count,3))\n",
    "print('Test set: Target=1:', int(np.sum(test_targets)), ' / Total # targets:', int(test_samples_count), ' / Percentage:',  np.round(np.sum(test_targets) / test_samples_count,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 6. Save the three datasets in *.npz & Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the three datasets in *.npz.\n",
    "np.savez('Audiobooks_data_train', inputs=train_inputs, targets=train_targets)\n",
    "np.savez('Audiobooks_data_validation', inputs=validation_inputs, targets=validation_targets)\n",
    "np.savez('Audiobooks_data_test', inputs=test_inputs, targets=test_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Analysis'></a>\n",
    "## 3. Data Analysis \n",
    "&nbsp; 3. 1. Data <br>\n",
    "&nbsp; 3. 2. Model <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the relevant libraries to create \n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 1. Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the training set \n",
    "npz_train = np.load('Audiobooks_data_train.npz')\n",
    "# Training Inputs & Targets\n",
    "# Note that targets must be integer, because of sparse_categorical_crossentropy \n",
    "train_inputs, train_targets = npz_train['inputs'].astype(np.float), npz_train['targets'].astype(np.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the validation set \n",
    "npz_valid = np.load('Audiobooks_data_validation.npz')\n",
    "# Inputs & Targets in the validatin set\n",
    "validation_inputs, validation_targets = npz_valid['inputs'].astype(np.float), npz_valid['targets'].astype(np.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the test dataset\n",
    "npz_test = np.load('Audiobooks_data_test.npz')\n",
    "# # Inputs & Targets in the test set\n",
    "test_inputs, test_targets = npz_test['inputs'].astype(np.float), npz_test['targets'].astype(np.int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 2. Model\n",
    "#### Outline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the input and output sizes\n",
    "input_size = 10\n",
    "output_size = 2\n",
    "# Set the same hidden layer size for both hidden layers\n",
    "hidden_layer_size = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Since there are 10 predictors in csv, and the target with 2 classes in the provided dataset. These decided the size of inputs and outputs. 50 Hidden units provide enough complexity. I did not put too many units at the begining for better speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model \n",
    "model_audiobook = tf.keras.Sequential([\n",
    "                            # 1st hidden layer \n",
    "                            tf.keras.layers.Dense(hidden_layer_size, activation='relu'), \n",
    "                            # 2nd hidden layer\n",
    "                            tf.keras.layers.Dense(hidden_layer_size, activation='relu'), \n",
    "                            # Output layer with softmax function\n",
    "                            tf.keras.layers.Dense(output_size, activation='softmax') # output layer\n",
    "                            ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Since the data is already preprocessed, it is not necessary to include input layer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimizer and Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_audiobook.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> I defined the optimizer: 'adam', the loss function 'Sparse Categorical Cross-Entropy' and the metrics, obtaining at each iteration.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Early Stopping Mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I set patience=2, to be a bit tolerant against random validation loss increases\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(patience=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the batch size\n",
    "batch_size = 100\n",
    "\n",
    "# Set a maximum number of training epochs\n",
    "max_epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "36/36 - 0s - loss: 0.6119 - accuracy: 0.6695 - val_loss: 0.5309 - val_accuracy: 0.7293\n",
      "Epoch 2/100\n",
      "36/36 - 0s - loss: 0.4802 - accuracy: 0.7667 - val_loss: 0.4556 - val_accuracy: 0.7651\n",
      "Epoch 3/100\n",
      "36/36 - 0s - loss: 0.4211 - accuracy: 0.7857 - val_loss: 0.4197 - val_accuracy: 0.7651\n",
      "Epoch 4/100\n",
      "36/36 - 0s - loss: 0.3911 - accuracy: 0.7994 - val_loss: 0.3995 - val_accuracy: 0.7696\n",
      "Epoch 5/100\n",
      "36/36 - 0s - loss: 0.3750 - accuracy: 0.7974 - val_loss: 0.3875 - val_accuracy: 0.7852\n",
      "Epoch 6/100\n",
      "36/36 - 0s - loss: 0.3637 - accuracy: 0.8078 - val_loss: 0.3801 - val_accuracy: 0.7830\n",
      "Epoch 7/100\n",
      "36/36 - 0s - loss: 0.3587 - accuracy: 0.8030 - val_loss: 0.3747 - val_accuracy: 0.7987\n",
      "Epoch 8/100\n",
      "36/36 - 0s - loss: 0.3528 - accuracy: 0.8061 - val_loss: 0.3701 - val_accuracy: 0.7987\n",
      "Epoch 9/100\n",
      "36/36 - 0s - loss: 0.3471 - accuracy: 0.8167 - val_loss: 0.3641 - val_accuracy: 0.8054\n",
      "Epoch 10/100\n",
      "36/36 - 0s - loss: 0.3441 - accuracy: 0.8136 - val_loss: 0.3610 - val_accuracy: 0.8054\n",
      "Epoch 11/100\n",
      "36/36 - 0s - loss: 0.3423 - accuracy: 0.8150 - val_loss: 0.3569 - val_accuracy: 0.7987\n",
      "Epoch 12/100\n",
      "36/36 - 0s - loss: 0.3375 - accuracy: 0.8145 - val_loss: 0.3530 - val_accuracy: 0.7987\n",
      "Epoch 13/100\n",
      "36/36 - 0s - loss: 0.3353 - accuracy: 0.8178 - val_loss: 0.3566 - val_accuracy: 0.8031\n",
      "Epoch 14/100\n",
      "36/36 - 0s - loss: 0.3349 - accuracy: 0.8220 - val_loss: 0.3588 - val_accuracy: 0.8098\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2053736e970>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note that the train, validation and test data are not iterable in this time\n",
    "model_audiobook.fit(train_inputs,                                            # 1. train inputs\n",
    "                    train_targets,                                           # 2. train targets\n",
    "                    batch_size=batch_size,                                   # 3. batch size\n",
    "                    epochs=max_epochs,                                       # 4. epochs that I train for \n",
    "                    callbacks=[early_stopping],                              # 5. early stopping\n",
    "                    validation_data=(validation_inputs, validation_targets), # 6. validation data\n",
    "                    verbose = 2)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 'callbacks' is the function called by a task when a task is completed. The task here is to check whether val_loss is increasing. After 12 epochs of training, I have reached a validation accuracy of ca. 82%. Since I set an early stopping mechanism, the training didnt go through for all epochs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Test'></a>\n",
    "## 4. Test the Model\n",
    "> After fitting on the training data and validating on the validation data, I tested the final prediction power of the model by running it on the test dataset which the algorithm has NEVER used before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - 0s 924us/step - loss: 0.3283 - accuracy: 0.8438\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model_audiobook.evaluate(test_inputs, test_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test loss: 0.33. Test accuracy: 84.38%\n"
     ]
    }
   ],
   "source": [
    "print('\\nTest loss: {0:.2f}. Test accuracy: {1:.2f}%'.format(test_loss, test_accuracy*100.))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The final accuracy is very close to the validation accuracy, since I did not fiddle too much with hyperparameters. Using the initial model and hyperparameters given in this notebook, the final test accuracy should be roughly around 80%. Again, note that each time the code is rerun, a different accuracy will be obtained because each training is different. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py3-TF2.0]",
   "language": "python",
   "name": "conda-env-py3-TF2.0-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
